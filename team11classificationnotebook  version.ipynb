{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TEAM 11 CLASSIFICATION: CLIMATE CHANGE TWEET SENTIMENT ANALYSIS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-05T12:42:55.171079Z","iopub.execute_input":"2021-12-05T12:42:55.171753Z","iopub.status.idle":"2021-12-05T12:43:04.123826Z","shell.execute_reply.started":"2021-12-05T12:42:55.171613Z","shell.execute_reply":"2021-12-05T12:43:04.122901Z"}}},{"cell_type":"code","source":"#Installing the comet library for version control\n!pip install comet_ml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by importing Comet and starting an experiment in Comet so that we can save the different experiments and models we run.This is for effective version control.","metadata":{}},{"cell_type":"code","source":"# import comet_ml at the top of your file\nfrom comet_ml import Experiment\n\n# Create an experiment with your api key\nexperiment = Experiment(\n    api_key=\"QTucqdVPpqdILbXpZnSCKY6vP\",\n    project_name=\"unsupervised-classification-predict\",\n    workspace=\"euphrasiam\",\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:04.126179Z","iopub.execute_input":"2021-12-05T12:43:04.126660Z","iopub.status.idle":"2021-12-05T12:43:09.410391Z","shell.execute_reply.started":"2021-12-05T12:43:04.126610Z","shell.execute_reply":"2021-12-05T12:43:09.409323Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Importing the packages needed for data engineering **","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk import TreebankWordTokenizer, SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport urllib\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nimport re\nimport re\nimport nltk\nimport itertools\nimport seaborn as sns\nimport warnings\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:00:42.833265Z","iopub.execute_input":"2021-12-05T11:00:42.834456Z","iopub.status.idle":"2021-12-05T11:00:44.913162Z","shell.execute_reply.started":"2021-12-05T11:00:42.834371Z","shell.execute_reply":"2021-12-05T11:00:44.912448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing our data i.e cvs files","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom pathlib import Path\ndata = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data[filename] = Path(dirname) / filename\n\n# Any results you write to the current directory are saved as output.\ndata","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:14.401436Z","iopub.execute_input":"2021-12-05T12:43:14.401722Z","iopub.status.idle":"2021-12-05T12:43:14.417959Z","shell.execute_reply.started":"2021-12-05T12:43:14.401692Z","shell.execute_reply":"2021-12-05T12:43:14.416915Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from IPython.display import (\n    Markdown as md,\n    Latex,\n    HTML,\n)\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:19.989080Z","iopub.execute_input":"2021-12-05T12:43:19.989430Z","iopub.status.idle":"2021-12-05T12:43:19.994727Z","shell.execute_reply.started":"2021-12-05T12:43:19.989381Z","shell.execute_reply":"2021-12-05T12:43:19.993630Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ntweets = pd.read_csv(\"../input/20212022-climate-change-competition/train.csv\")\ntest_data = pd.read_csv(\"../input/20212022-climate-change-competition/test_with_no_labels.csv\")\n\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:24.348250Z","iopub.execute_input":"2021-12-05T12:43:24.348567Z","iopub.status.idle":"2021-12-05T12:43:24.469200Z","shell.execute_reply.started":"2021-12-05T12:43:24.348533Z","shell.execute_reply":"2021-12-05T12:43:24.468209Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We see here that our train data set has 3 columns, the tweetid, the message and the sentiment observed","metadata":{}},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)\n\n\nWe're going to start an Exploratory Data Analysis (EDA). The first step of any Machine Learning project is to develop an understanding of your data, as that will help with model selection later on.","metadata":{}},{"cell_type":"code","source":"#display the shape of our dataframe created from out train dataset\ndisplay(tweets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:28.285861Z","iopub.execute_input":"2021-12-05T12:43:28.286141Z","iopub.status.idle":"2021-12-05T12:43:28.291966Z","shell.execute_reply.started":"2021-12-05T12:43:28.286112Z","shell.execute_reply":"2021-12-05T12:43:28.291316Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"We note here that our datframe has 3 columns and 15819 rows","metadata":{}},{"cell_type":"code","source":"#checking the value counts for the sentiment row\nvalue_counts = tweets[\"sentiment\"].value_counts()\nvalue_counts.name = \"Raw Number\"\n\nvalue_normd = tweets[\"sentiment\"].value_counts(normalize=True)\nvalue_normd.name = \"Percentage\"\n\ndisplay(pd.concat([value_counts, value_normd], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:31.934684Z","iopub.execute_input":"2021-12-05T12:43:31.935366Z","iopub.status.idle":"2021-12-05T12:43:31.953041Z","shell.execute_reply.started":"2021-12-05T12:43:31.935326Z","shell.execute_reply":"2021-12-05T12:43:31.952119Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"We note here above what percentage of our sampled data is allocated to each sentiment. Like 1 being 53%,2 23% etc","metadata":{}},{"cell_type":"markdown","source":"Another important thing to note when doing any data engineering and modelling would be the datatypes","metadata":{}},{"cell_type":"code","source":"#view datatype\ntweets.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:35.801622Z","iopub.execute_input":"2021-12-05T12:43:35.802280Z","iopub.status.idle":"2021-12-05T12:43:35.820909Z","shell.execute_reply.started":"2021-12-05T12:43:35.802243Z","shell.execute_reply":"2021-12-05T12:43:35.820253Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Our sentiment and tweetid columns are of numeric type while the message is of object type","metadata":{}},{"cell_type":"markdown","source":"Let's make a copy of the pd.DataFrame so we can feed tweets into our models later.","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\neda = deepcopy(tweets)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:39.800869Z","iopub.execute_input":"2021-12-05T12:43:39.801509Z","iopub.status.idle":"2021-12-05T12:43:39.806249Z","shell.execute_reply.started":"2021-12-05T12:43:39.801456Z","shell.execute_reply":"2021-12-05T12:43:39.805479Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Here we replace the numeral values for sentiment with text so as to make our analysis easier to understand","metadata":{}},{"cell_type":"code","source":"sentiment_num2name = {\n    -1: \"Anti\",\n     0: \"Neutral\",\n     1: \"Pro\",\n     2: \"News\",\n}\neda[\"sentiment\"] = eda[\"sentiment\"].apply(lambda num: sentiment_num2name[num])\neda.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:42.831454Z","iopub.execute_input":"2021-12-05T12:43:42.831724Z","iopub.status.idle":"2021-12-05T12:43:42.851857Z","shell.execute_reply.started":"2021-12-05T12:43:42.831696Z","shell.execute_reply":"2021-12-05T12:43:42.850773Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Graphically visualize how the sentiment is distributed in the data","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=1.5) #setting font types and which style we want to use\nstyle.use(\"seaborn-poster\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:46.686134Z","iopub.execute_input":"2021-12-05T12:43:46.686568Z","iopub.status.idle":"2021-12-05T12:43:46.695107Z","shell.execute_reply.started":"2021-12-05T12:43:46.686535Z","shell.execute_reply":"2021-12-05T12:43:46.693971Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#plotting our data in a pie chart and bar graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), dpi=100)\n\nsns.countplot(eda[\"sentiment\"], ax=axes[0])\nlabels = list(sentiment_num2name.values())\n\naxes[1].pie(eda[\"sentiment\"].value_counts(),\n            labels=['Pro','News','Neutral','Anti'],\n            autopct=\"%1.0f%%\",\n            startangle=90,\n            explode=tuple([0.1] * len(labels)))\n\nfig.suptitle(\"Distribution of Tweets\", fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:55:43.722969Z","iopub.execute_input":"2021-12-05T14:55:43.724045Z","iopub.status.idle":"2021-12-05T14:55:44.147918Z","shell.execute_reply.started":"2021-12-05T14:55:43.723992Z","shell.execute_reply":"2021-12-05T14:55:44.146595Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"What we see here graphically is what the table abpve depicted, which is that the pro climate change sentiment is the highest, followed by the news then the neutral an lastly the Anti climate cnahge sentiment.","metadata":{}},{"cell_type":"markdown","source":"What we look into next is creating a table that will show us the top 15 hashtags for each sentiment class.This may give us a better idea about the sentiments people have just from seeing their hashtags","metadata":{}},{"cell_type":"code","source":"top15 = {}\n\nby_sentiment = eda.groupby(\"sentiment\")\nfor sentiment, group in tqdm(by_sentiment):\n    hashtags = group[\"message\"].apply(lambda tweet: re.findall(r\"#(\\w+)\", tweet))\n    hashtags = itertools.chain(*hashtags)\n    hashtags = [ht.lower() for ht in hashtags]\n    \n    frequency = nltk.FreqDist(hashtags)\n    \n    df_hashtags = pd.DataFrame({\n        \"hashtags\": list(frequency.keys()),\n        \"counts\": list(frequency.values()),\n    })\n    top15_htags = df_hashtags.nlargest(15, columns=[\"counts\"])\n    \n    top15[sentiment] = top15_htags.reset_index(drop=True)\n\ndisplay(pd.concat(top15, axis=1).head(n=10))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:43:54.517294Z","iopub.execute_input":"2021-12-05T12:43:54.517604Z","iopub.status.idle":"2021-12-05T12:43:54.647429Z","shell.execute_reply.started":"2021-12-05T12:43:54.517573Z","shell.execute_reply":"2021-12-05T12:43:54.646824Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"You will notice that the most commond hastags revolve around the Trump campaign, voting and climate change policy. This tells us that this dataset was sampled from during campiagn times","metadata":{}},{"cell_type":"markdown","source":"**Anti**:From here we pick up various hastags that actually makes sense for the sentiment they represent.For instance under the Anti columns we have hashtags like mada(Make America Great Again) which the slogan that Donald Trump used in his campaign and we know that Trump was very anti-climate change.We also have fakenews as a trending hashtag under Anti as Trump and his supporters used that statement alot when trying to nullify anything in the news they did not agree with. More so global warming and climate change.Another interesting one is \"opchemtrails\" one; this refers to operation chemtrails, a conspiracy theory that anti climate change activists believe that heads of States are lacing our atmosphere with harmful gasses so that they can advance the climate change narrative","metadata":{}},{"cell_type":"markdown","source":"**Pro**: The pro sentiment has trending hastags like beforetheflood(A movie about climate change and the global action needed to fight it.Another one that stands out is Cop22(The 2016 UN climate change conference held in Morocco where they discussed various enviromental issues) and also parisagreement(is an international treaty on climate change, adopted in 2015. It covers climate change mitigation, adaptation, and finance).These 2 a big reference point for people who believe in climate change.","metadata":{}},{"cell_type":"markdown","source":"**News** The news columns hashtags cover cop22 and the greatbarrierreefs, mainly because 2016 included headlines like \"Extreme heat in 2016 damaged Australia's Great Barrier Reef much more substantially than initial surveys indicated\" which also informed trends in other sentiment classes","metadata":{}},{"cell_type":"markdown","source":"**Neutral** Some interesting hashtags that stand out for us are \"qanda\" and \"amreading\" hastags that makes sense for the neutral sentiment which show that this is mainly people who are neutral and doing their research on the issues. Also the other hastags represent bothe the pro and anti sentiment classes.","metadata":{}},{"cell_type":"markdown","source":"We now want to look at the popularity of these hashtags visually per the sentiment classes","metadata":{}},{"cell_type":"code","source":"#fig, axes = plt.subplots(2, 2, figsize=(eda[\"message\"].apply(cleaner), 20))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:34:37.217702Z","iopub.execute_input":"2021-12-05T14:34:37.218228Z","iopub.status.idle":"2021-12-05T14:34:37.223334Z","shell.execute_reply.started":"2021-12-05T14:34:37.218167Z","shell.execute_reply":"2021-12-05T14:34:37.222650Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#fig, axes = plt.subplots(2, 2, figsize=(eda[\"message\"] = eda[\"message\"].apply(cleaner),eda.head()28, 20))\n#counter = 0\n\n#for sentiment, top in top15.items():\n   # sns.barplot(data=top, y=\"hashtags\", x=\"counts\", palette=\"Blues_d\", ax=axes[counter // 2, counter % 2])\n   # axes[counter // 2, counter % 2].set_title(f\"Most frequent Hashtags by {sentiment} (Visually)\", fontsize=25)\n   # counter += 1\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:35:09.777465Z","iopub.execute_input":"2021-12-05T14:35:09.777770Z","iopub.status.idle":"2021-12-05T14:35:09.783042Z","shell.execute_reply.started":"2021-12-05T14:35:09.777737Z","shell.execute_reply":"2021-12-05T14:35:09.782015Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Data Engineering","metadata":{}},{"cell_type":"code","source":"#This function is meant to remove all the noise in the tweets so that we can be able \n#to strictly analyse the words in the tweets against our target variable\ndef cleaner(tweet):\n    tweet = tweet.lower()\n    \n    to_del = [\n        r\"@[\\w]*\",  # strip account mentions\n        r\"http(s?):\\/\\/.*\\/\\w*\",  # strip URLs\n        r\"#\\w*\",  # strip hashtags\n        r\"\\d+\",  # delete numeric values\n        r\"rt[\\s]+\",\n        r\"U+FFFD\",  # remove the \"character note present\" diamond\n    ]\n    for key in to_del:\n        tweet = re.sub(key, \"\", tweet)\n    \n    # strip punctuation and special characters\n    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", \" \", tweet)\n    # strip excess white-space\n    tweet = re.sub(r\"\\s\\s+\", \" \", tweet)\n    \n    return tweet.lstrip(\" \")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:11:24.255803Z","iopub.execute_input":"2021-12-05T13:11:24.256124Z","iopub.status.idle":"2021-12-05T13:11:24.264004Z","shell.execute_reply.started":"2021-12-05T13:11:24.256093Z","shell.execute_reply":"2021-12-05T13:11:24.263236Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#We apply our cleaner function to the eda dataframe that we copied\neda[\"message\"] = eda[\"message\"].apply(cleaner)\neda.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:18:50.122203Z","iopub.execute_input":"2021-12-05T13:18:50.122576Z","iopub.status.idle":"2021-12-05T13:18:50.579311Z","shell.execute_reply.started":"2021-12-05T13:18:50.122540Z","shell.execute_reply":"2021-12-05T13:18:50.578362Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"As you can see all the punctiation,caps and extra spaces are gone and we just have plain text that we can analyze","metadata":{}},{"cell_type":"code","source":"# Importing some libraries for NLP data engineering the text\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords, wordnet  ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:28:40.691175Z","iopub.execute_input":"2021-12-05T13:28:40.691532Z","iopub.status.idle":"2021-12-05T13:28:40.697587Z","shell.execute_reply.started":"2021-12-05T13:28:40.691493Z","shell.execute_reply":"2021-12-05T13:28:40.696449Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#Function to lammatize the data frame\n#This function also tokenizes and gives us the different parts of speech for the words\n\ndef lemmatizer(df):\n    df[\"length\"] = df[\"message\"].str.len()\n    df[\"tokenized\"] = df[\"message\"].apply(word_tokenize)\n    df[\"parts-of-speech\"] = df[\"tokenized\"].apply(nltk.tag.pos_tag)\n    \n    def str2wordnet(tag):\n        conversion = {\"J\": wordnet.ADJ, \"V\": wordnet.VERB, \"N\": wordnet.NOUN, \"R\": wordnet.ADV}\n        try:\n            return conversion[tag[0].upper()]\n        except KeyError:\n            return wordnet.NOUN\n    \n    wnl = WordNetLemmatizer()\n    df[\"parts-of-speech\"] = df[\"parts-of-speech\"].apply(\n        lambda tokens: [(word, str2wordnet(tag)) for word, tag in tokens]\n    )\n    df[\"lemmatized\"] = df[\"parts-of-speech\"].apply(\n        lambda tokens: [wnl.lemmatize(word, tag) for word, tag in tokens]\n    )\n    df[\"lemmatized\"] = df[\"lemmatized\"].apply(lambda tokens: \" \".join(map(str, tokens)))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:29:21.268810Z","iopub.execute_input":"2021-12-05T13:29:21.269140Z","iopub.status.idle":"2021-12-05T13:29:21.279424Z","shell.execute_reply.started":"2021-12-05T13:29:21.269095Z","shell.execute_reply":"2021-12-05T13:29:21.278547Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#lemmatize the eda dataframe\neda = lemmatizer(eda)\neda.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:30:37.487225Z","iopub.execute_input":"2021-12-05T13:30:37.488625Z","iopub.status.idle":"2021-12-05T13:31:03.329274Z","shell.execute_reply.started":"2021-12-05T13:30:37.488579Z","shell.execute_reply":"2021-12-05T13:31:03.328458Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# More EDA: buzzwords","metadata":{}},{"cell_type":"code","source":"#Checking what the buzzwords are, that is the words that show up the most in the tweets.\n#We use countvectorizer to remove English stop words \nfrequency = {}\n\nby_sentiment = eda.groupby(\"sentiment\")\nfor sentiment, group in tqdm(by_sentiment):\n    cv = CountVectorizer(stop_words=\"english\")\n    words = cv.fit_transform(group[\"lemmatized\"])\n    \n    n_words = words.sum(axis=0)\n    word_freq = [(word, n_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n    \n    freq = pd.DataFrame(word_freq, columns=[\"word\", \"freq\"])\n    \n    frequency[sentiment] = freq.head(n=25)\n\nto_view = pd.concat(frequency, axis=1).head(n=25)\ndisplay(to_view)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:39:34.559882Z","iopub.execute_input":"2021-12-05T13:39:34.560215Z","iopub.status.idle":"2021-12-05T13:39:35.118274Z","shell.execute_reply.started":"2021-12-05T13:39:34.560181Z","shell.execute_reply":"2021-12-05T13:39:35.117326Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Now that we have computed the frequencies of these buzzwords we use Word clouds to visually see what is happening with these word frequencies","metadata":{}},{"cell_type":"code","source":"words = {sentiment: \" \".join(frequency[sentiment][\"word\"].values) for sentiment in sentiment_num2name.values()}\n\ncmaps = {\n    \"Anti\": (\"Reds\", 110),\n    \"Pro\" : (\"Greens\", 73),\n    \"News\": (\"Blues\", 0),\n    \"Neutral\": (\"Oranges\", 10),\n}\n\nfrom wordcloud import WordCloud\n\nwordclouds = {}\nfor sentiment, (cmap, rand) in tqdm(cmaps.items()):\n    wordclouds[sentiment] = WordCloud(\n        width=800, height=500, random_state=rand,\n        max_font_size=110, background_color=\"white\",\n        colormap=cmap\n    ).generate(words[sentiment])\n    \nfig, axes = plt.subplots(2, 2, figsize=(28, 20))\ncounter = 0\n\nfor sentiment, wordcloud in wordclouds.items():\n    axes[counter // 2, counter % 2].imshow(wordcloud)\n    axes[counter // 2, counter % 2].set_title(sentiment, fontsize=25)\n    counter += 1\n    \nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:43:03.630427Z","iopub.execute_input":"2021-12-05T13:43:03.630819Z","iopub.status.idle":"2021-12-05T13:43:05.652566Z","shell.execute_reply.started":"2021-12-05T13:43:03.630778Z","shell.execute_reply":"2021-12-05T13:43:05.651586Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"* Words like science ,data and scientist are occuring alot under Anti, which could indicate that they either believe they are quoting research or could mean they are quite against the opinions shared by scientists. Other interesting words coming out from the Anti wordcloud are scam,fake, tax and hoax which speak to the core arguments of this sentiment.That the climate change movement is meant to raise taxes and that it is a scam\n* Words like Climate, Change, global, trump are common for all the word clouds and this could be because this is the subject matter of all the sentiments\n* The news word cloud has words that point towards current affairs like the paris agreement, moving from Obama to Trump administration,the EPA and Pruitt(Environmental Protection Agency Administrator Scott Pruitt)\n* Words like believe, think,cause,real speak to how strongly the pro sentiment class belive in the climate change movement and that perhaps those who don't are not 'thinking'","metadata":{}},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"# Application of the function to clean the tweets in the train and test datasets\ntweets['message'] = tweets['message'].apply(cleaner)\ntest_data['message'] = test_data['message'].apply(cleaner)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:00:53.695728Z","iopub.execute_input":"2021-12-05T14:00:53.696697Z","iopub.status.idle":"2021-12-05T14:00:54.450570Z","shell.execute_reply.started":"2021-12-05T14:00:53.696652Z","shell.execute_reply":"2021-12-05T14:00:54.449788Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#Apply the lemmatizer function to the test and train dataset before modelling\ntweets = lemmatizer(tweets)\ntest_data = lemmatizer(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:01:31.718658Z","iopub.execute_input":"2021-12-05T14:01:31.719041Z","iopub.status.idle":"2021-12-05T14:02:10.051625Z","shell.execute_reply.started":"2021-12-05T14:01:31.719003Z","shell.execute_reply":"2021-12-05T14:02:10.050663Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#separate the target and feature variables\nX = tweets['message']\ny = tweets['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:02:56.917434Z","iopub.execute_input":"2021-12-05T14:02:56.917747Z","iopub.status.idle":"2021-12-05T14:02:56.923896Z","shell.execute_reply.started":"2021-12-05T14:02:56.917705Z","shell.execute_reply":"2021-12-05T14:02:56.922547Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#Train test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:03:02.149797Z","iopub.execute_input":"2021-12-05T14:03:02.150150Z","iopub.status.idle":"2021-12-05T14:03:02.161768Z","shell.execute_reply.started":"2021-12-05T14:03:02.150109Z","shell.execute_reply":"2021-12-05T14:03:02.160473Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Importing Packages for training models\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\n\n\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport time\n\n# Model Evaluation Packages\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom sklearn.metrics import make_scorer\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:03:04.933636Z","iopub.execute_input":"2021-12-05T14:03:04.933980Z","iopub.status.idle":"2021-12-05T14:03:04.948581Z","shell.execute_reply.started":"2021-12-05T14:03:04.933943Z","shell.execute_reply":"2021-12-05T14:03:04.947669Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"THE FIVE DIFFERENT MODELS WE TRY OUT ARE:\n1. Linear SVC Model\n2. Logistics Regression\n3. K Neighbours Classifier\n4. Multinomial Naive Bayers\n5. SGD classifier\n6. Complement Naive Bayers classifier","metadata":{}},{"cell_type":"markdown","source":"1. LINEAR SVC MODEL","metadata":{}},{"cell_type":"code","source":"#linear SVC model\nLsvc=LinearSVC(random_state=42)\n#fit the model\nclf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', Lsvc)])\n\n        # Logging the Execution Time for each model\nstart_time = time.time()\nclf_text.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nLsvc_pred = clf_text.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:11:09.403451Z","iopub.execute_input":"2021-12-02T01:11:09.403932Z","iopub.status.idle":"2021-12-02T01:11:10.832439Z","shell.execute_reply.started":"2021-12-02T01:11:09.403881Z","shell.execute_reply":"2021-12-02T01:11:10.831484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,Lsvc_pred,average='macro')\nF1_Accuracy=f1_score(y_val, Lsvc_pred,average='micro')\nF1_Weighted=f1_score(y_val,Lsvc_pred,average='weighted')\nExecution_Time=run_time","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:19:50.25406Z","iopub.execute_input":"2021-12-02T01:19:50.254432Z","iopub.status.idle":"2021-12-02T01:19:50.267598Z","shell.execute_reply.started":"2021-12-02T01:19:50.254397Z","shell.execute_reply":"2021-12-02T01:19:50.266805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":42,\n       \"Model_type\": \"LinearSVC\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:22:56.103954Z","iopub.execute_input":"2021-12-02T01:22:56.104264Z","iopub.status.idle":"2021-12-02T01:22:56.109544Z","shell.execute_reply.started":"2021-12-02T01:22:56.104234Z","shell.execute_reply":"2021-12-02T01:22:56.108498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params) #logging metrics for experiment\nexperiment.log_metrics(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:23:01.439417Z","iopub.execute_input":"2021-12-02T01:23:01.440045Z","iopub.status.idle":"2021-12-02T01:23:01.445349Z","shell.execute_reply.started":"2021-12-02T01:23:01.44Z","shell.execute_reply":"2021-12-02T01:23:01.444723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:23:06.594571Z","iopub.execute_input":"2021-12-02T01:23:06.594879Z","iopub.status.idle":"2021-12-02T01:23:08.279292Z","shell.execute_reply.started":"2021-12-02T01:23:06.594845Z","shell.execute_reply":"2021-12-02T01:23:08.27852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"lr=LogisticRegression(random_state=42,\n                                  multi_class='ovr',\n                                  n_jobs=1,\n                                  C=1e5,\n                                  max_iter=4000)\n#fit the model\nclf_lr = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', lr)])\n\n        # Logging the Execution Time for each model\nstart_time = time.time()\nclf_lr.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nlr_pred = clf_lr.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:41:37.038761Z","iopub.execute_input":"2021-12-02T01:41:37.039059Z","iopub.status.idle":"2021-12-02T01:43:39.772957Z","shell.execute_reply.started":"2021-12-02T01:41:37.039025Z","shell.execute_reply":"2021-12-02T01:43:39.772342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,lr_pred,average='macro')\nF1_Accuracy=f1_score(y_val, lr_pred,average='micro')\nF1_Weighted=f1_score(y_val,lr_pred,average='weighted')\nExecution_Time=run_time","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:43:47.908762Z","iopub.execute_input":"2021-12-02T01:43:47.909317Z","iopub.status.idle":"2021-12-02T01:43:47.921197Z","shell.execute_reply.started":"2021-12-02T01:43:47.909268Z","shell.execute_reply":"2021-12-02T01:43:47.920524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":42,\n       \"Model_type\": \"Logistic Regression\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:46:01.080629Z","iopub.execute_input":"2021-12-02T01:46:01.080946Z","iopub.status.idle":"2021-12-02T01:46:01.086056Z","shell.execute_reply.started":"2021-12-02T01:46:01.08091Z","shell.execute_reply":"2021-12-02T01:46:01.085207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params) #logging metrics for experiment\nexperiment.log_metrics(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:46:05.170439Z","iopub.execute_input":"2021-12-02T01:46:05.171203Z","iopub.status.idle":"2021-12-02T01:46:05.175939Z","shell.execute_reply.started":"2021-12-02T01:46:05.171159Z","shell.execute_reply":"2021-12-02T01:46:05.175358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:46:08.480867Z","iopub.execute_input":"2021-12-02T01:46:08.481887Z","iopub.status.idle":"2021-12-02T01:46:10.370464Z","shell.execute_reply.started":"2021-12-02T01:46:08.481846Z","shell.execute_reply":"2021-12-02T01:46:10.369566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. K NEIGHBOURS CLASSIFIER","metadata":{}},{"cell_type":"code","source":"kn=KNeighborsClassifier(n_neighbors=5)\n#fit the model\nclf_kn = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', kn)])\n\n# Logging the Execution Time for each model\nstart_time = time.time()\nclf_kn.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nkn_pred = clf_kn.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:51:47.216299Z","iopub.execute_input":"2021-12-02T01:51:47.216953Z","iopub.status.idle":"2021-12-02T01:51:49.565198Z","shell.execute_reply.started":"2021-12-02T01:51:47.216908Z","shell.execute_reply":"2021-12-02T01:51:49.564279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,kn_pred,average='macro')\nF1_Accuracy=f1_score(y_val, kn_pred,average='micro')\nF1_Weighted=f1_score(y_val,kn_pred,average='weighted')\nExecution_Time=run_time","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:52:23.841159Z","iopub.execute_input":"2021-12-02T01:52:23.841477Z","iopub.status.idle":"2021-12-02T01:52:23.851787Z","shell.execute_reply.started":"2021-12-02T01:52:23.841443Z","shell.execute_reply":"2021-12-02T01:52:23.851087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":42,\n       \"Model_type\": \"K-Neighbours\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:52:45.391114Z","iopub.execute_input":"2021-12-02T01:52:45.391555Z","iopub.status.idle":"2021-12-02T01:52:45.397424Z","shell.execute_reply.started":"2021-12-02T01:52:45.391526Z","shell.execute_reply":"2021-12-02T01:52:45.39657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params) #logging metrics for experiment\nexperiment.log_metrics(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:52:55.896347Z","iopub.execute_input":"2021-12-02T01:52:55.896762Z","iopub.status.idle":"2021-12-02T01:52:55.901494Z","shell.execute_reply.started":"2021-12-02T01:52:55.896733Z","shell.execute_reply":"2021-12-02T01:52:55.900668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:52:59.318356Z","iopub.execute_input":"2021-12-02T01:52:59.318643Z","iopub.status.idle":"2021-12-02T01:52:59.728243Z","shell.execute_reply.started":"2021-12-02T01:52:59.318615Z","shell.execute_reply":"2021-12-02T01:52:59.727382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. MULTINOMIAL NB","metadata":{}},{"cell_type":"code","source":"mnb=MultinomialNB()\n#fit the model\nclf_mnb = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', mnb)])\n\n# Logging the Execution Time for each model\nstart_time = time.time()\nclf_mnb.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nmnb_pred = clf_mnb.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,mnb_pred,average='macro')\nF1_Accuracy=f1_score(y_val, mnb_pred,average='micro')\nF1_Weighted=f1_score(y_val,mnb_pred,average='weighted')\nExecution_Time=run_time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":\"default\",\n       \"Model_type\": \"Multinomial\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params)\nexperiment.log_metrics(metrics) #logging metrics for experiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end() # ending the comet experiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. SGD CLASSIFIER","metadata":{}},{"cell_type":"code","source":"sgd=SGDClassifier(loss='hinge',penalty='l2',\n                             alpha=1e-3,\n                             random_state=42,\n                             max_iter=5,\n                             tol=None)\n#fit the model\nclf_sdg = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', sdg)])\n\n# Logging the Execution Time for each model\nstart_time = time.time()\nclf_sdg.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nsdg_pred = clf_sdg.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,mnb_pred,average='macro')\nF1_Accuracy=f1_score(y_val, mnb_pred,average='micro')\nF1_Weighted=f1_score(y_val,mnb_pred,average='weighted')\nExecution_Time=run_time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":\"default\",\n       \"Model_type\": \"Multinomial\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params) #logging metrics for experiment\nexperiment.log_metrics(metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.COMPLEMENT NB CLASSIFIER","metadata":{}},{"cell_type":"code","source":"cn=ComplementNB()\n#fit the model\nclf_sdg = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                               max_df=0.9,\n                                               ngram_range=(1, 2))),\n                     ('clf', sdg)])\n\n# Logging the Execution Time for each model\nstart_time = time.time()\nclf_sdg.fit(X_train, y_train)\nrun_time = time.time()-start_time\n\nsdg_pred = clf_sdg.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving each metric to a dictionary for logging\nF1_Macro=f1_score(y_val,mnb_pred,average='macro')\nF1_Accuracy=f1_score(y_val, mnb_pred,average='micro')\nF1_Weighted=f1_score(y_val,mnb_pred,average='weighted')\nExecution_Time=run_time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dictionaries for the data you want to log\nparams={\"Random state\":\"default\",\n       \"Model_type\": \"Multinomial\",\n       }\n\nmetrics={\"F1_Macro\":F1_Macro,\n\"F1_Accuracy\":F1_Accuracy,\n\"F1_Weighted\":F1_Weighted,\n\"Execution_Time\":Execution_Time\n    \n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.log_parameters(params)\nexperiment.log_metrics(metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end() #ending the comet experiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refining the train-test split for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T11:22:37.816792Z","iopub.execute_input":"2021-12-01T11:22:37.81712Z","iopub.status.idle":"2021-12-01T11:22:37.826364Z","shell.execute_reply.started":"2021-12-01T11:22:37.81708Z","shell.execute_reply":"2021-12-01T11:22:37.825579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally based on performance of the models we choose the Multinomial Naive Bias classifier and we predict the data of the validation set which we separated.","metadata":{}},{"cell_type":"code","source":"# Creating a pipeline for the gridsearch\nparam_grid = {'alpha': [0.1, 1, 5, 10]}  # setting parameter grid\n\ntuned_mnb = Pipeline([('tfidf', TfidfVectorizer(min_df=2,\n                                                max_df=0.9,\n                                                ngram_range=(1, 2))),\n                      ('mnb', GridSearchCV(MultinomialNB(),\n                                           param_grid=param_grid,\n                                           cv=5,\n                                           n_jobs=-1,\n                                           scoring='f1_weighted'))\n                      ])\n\ntuned_mnb.fit(X_train, y_train)  # Fitting the model\n\ny_pred_mnb = tuned_mnb.predict(X_val)  # predicting the fit on validation set\n\nprint(classification_report(y_val, y_pred_mnb))","metadata":{"execution":{"iopub.status.busy":"2021-12-01T11:22:40.641923Z","iopub.execute_input":"2021-12-01T11:22:40.6425Z","iopub.status.idle":"2021-12-01T11:22:44.493753Z","shell.execute_reply.started":"2021-12-01T11:22:40.64245Z","shell.execute_reply":"2021-12-01T11:22:44.492802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting sentiment for our test data and produce csv to submit to Kaggle","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(test_data['tweetid'])\nsubmission_df['sentiment'] = tuned_mnb.predict(test_data['message'])\nsubmission_df.to_csv('submission4.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T11:25:33.347121Z","iopub.execute_input":"2021-12-01T11:25:33.347459Z","iopub.status.idle":"2021-12-01T11:25:33.84194Z","shell.execute_reply.started":"2021-12-01T11:25:33.347411Z","shell.execute_reply":"2021-12-01T11:25:33.840871Z"},"trusted":true},"execution_count":null,"outputs":[]}]}